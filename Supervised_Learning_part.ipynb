{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNCjggC_JCLC"
   },
   "source": [
    "# **Customer Churn Prediction** ¶\n",
    "\n",
    "\n",
    "## **Problem Statement:**\n",
    "\n",
    "Develop a predictive model to identify customers at risk of churning from an investment bank, enabling proactive retention strategies to minimize customer loss and maximize revenue growth.\n",
    "\n",
    "\n",
    "## **About the Dataset**\n",
    "\n",
    "There are 14 columns/features and 10k rows/samples.\n",
    "\n",
    "**RowNumber**—corresponds to the record (row) number and has no effect on the output.\n",
    "\n",
    "**CustomerId**—contains random values and has no effect on customer leaving the bank.\n",
    "\n",
    "**Surname**—the surname of a customer has no impact on their decision to leave the bank.\n",
    "\n",
    "**CreditScore**—can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.\n",
    "\n",
    "**Geography**—a customer’s location can affect their decision to leave the bank.\n",
    "\n",
    "**Gender**—it’s interesting to explore whether gender plays a role in a customer leaving the bank.\n",
    "\n",
    "**Age**—this is certainly relevant, since older customers are less likely to leave their bank than younger ones.\n",
    "\n",
    "**Tenure**—refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.\n",
    "\n",
    "**Balance**—also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.\n",
    "\n",
    "**NumOfProducts**—refers to the number of products that a customer has purchased through the bank.\n",
    "\n",
    "**HasCrCard**—denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.\n",
    "\n",
    "**IsActiveMember**—active customers are less likely to leave the bank.\n",
    "\n",
    "**EstimatedSalary**—as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.\n",
    "\n",
    "**Exited**—whether or not the customer left the bank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsM-UZzFLU6K"
   },
   "source": [
    "## **KNN**\n",
    "\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple and effective machine learning technique that classifies data points by finding the K most similar instances to a new input and voting for the target class or value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PaiYMl810_bx"
   },
   "source": [
    "### **The most commonly used hyperparameters for K-Nearest Neighbors (KNN) algorithm:**\n",
    "\n",
    "n_neighbors: The number of nearest neighbors to consider when making a prediction. Increasing this number can improve the model's performance, but also increases the computation time.\n",
    "\n",
    "weights: The weight function used to calculate the distance between samples. Supported weights are 'uniform' (all points have equal weight) and 'distance' (points closer to the query point have higher weight).\n",
    "\n",
    "algorithm: The algorithm used to compute the nearest neighbors. Supported algorithms are 'brute' (exhaustive search), 'kd_tree' (k-d tree search), and 'ball_tree' (ball tree search).\n",
    "\n",
    "leaf_size: The number of samples in each leaf node of the k-d tree or ball tree. Increasing this number can improve the model's performance, but also increases the computation time.\n",
    "\n",
    "p: The power parameter for the Minkowski metric. When p=1, it is the Manhattan distance, and when p=2, it is the Euclidean distance.\n",
    "\n",
    "metric: The distance metric used to calculate the distance between samples. Supported metrics are 'minkowski' (Minkowski distance), 'euclidean' (Euclidean distance), 'manhattan' (Manhattan distance), and 'chebyshev' (Chebyshev distance).\n",
    "\n",
    "### **Here are some common values for these hyperparameters:**\n",
    "\n",
    "n_neighbors: 3, 5, 10, 20\n",
    "\n",
    "weights: 'uniform', 'distance'\n",
    "\n",
    "algorithm: 'brute', 'kd_tree', 'ball_tree'\n",
    "\n",
    "leaf_size: 10, 20, 30\n",
    "\n",
    "p: 1, 2\n",
    "\n",
    "metric: 'minkowski', 'euclidean', 'manhattan', 'chebyshev'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VBDLDV06LVR5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cM4BNUKTL1d1"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-e6wApAqL1lZ"
   },
   "outputs": [],
   "source": [
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qkO2K9I4L1r1",
    "outputId": "d95565df-2b88-4a50-8d89-87befe30f5cd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "data = pd.read_csv('Churn_data .csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "osdVP2q05m1v"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vabpuR-z5m4P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gXYD_VtT5m8M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is null?\n",
    "isnull = data.isnull().sum()\n",
    "isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ibZA46O545HW"
   },
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "X['Geography'] = le.fit_transform(X['Geography'])\n",
    "X['Gender'] = le.fit_transform(X['Gender'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "n__3wDIp45E2"
   },
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "selected_features = [\n",
    "    'CreditScore', 'Geography', 'Gender', 'Age',\n",
    "    'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "    'IsActiveMember', 'EstimatedSalary'\n",
    "]\n",
    "X = data[selected_features]\n",
    "y = data[['Exited']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXGEGbzC45Kr"
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']] = scaler.fit_transform(X[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j44gSfIV5Dc5"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    X, y, random_state=0, train_size=0.8\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUKfmfYe5Di5"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = KNeighborsClassifier(n_neighbors=2, metric='euclidean', weights='uniform', algorithm='auto', leaf_size=50, p=2)\n",
    "model.fit(train_X, train_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS2Q1a0T45NM"
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "val_prediction = model.predict(val_X)\n",
    "y_pred_proba = model.predict_proba(val_X)[:,1]\n",
    "accuracy = accuracy_score(val_y, val_prediction)\n",
    "print(f'Model accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g_36FlE5cIr"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(val_y, val_prediction))\n",
    "print(classification_report(val_y, val_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfUIhRrS-7zL"
   },
   "outputs": [],
   "source": [
    "auc = roc_auc_score(val_y, y_pred_proba)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcIiAzYb45Qi"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(model, 'churn_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mDoUgypvtYz"
   },
   "source": [
    "A Decision Tree Classifier is a type of supervised learning algorithm in machine learning. It works by creating a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. The tree is constructed by recursively partitioning the data into subsets based on the values of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5KPCvs5v1YE"
   },
   "source": [
    "### **The most commonly used hyperparameters for Decision Tree Classifier**:\n",
    "\n",
    "criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "\n",
    "max_depth: The maximum depth of the tree. Increasing this number can improve the model's performance, but also increases the risk of overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. Decreasing this number can lead to overfitting, while increasing it can lead to underfitting.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. Decreasing this number can lead to overfitting, while increasing it can lead to underfitting.\n",
    "\n",
    "max_features: The maximum number of features to consider at each split. Increasing this number can improve the model's performance, but also increases the computation time.\n",
    "\n",
    "random_state: The random seed used to shuffle the data before splitting it into training and testing sets. Setting this to a fixed value ensures reproducibility of the results.\n",
    "\n",
    "class_weight: The weight assigned to each class during training. This can be useful for imbalanced datasets, where one class has a much larger number of instances than the others.\n",
    "\n",
    "### **Here are some common values for these hyperparameters:**\n",
    "\n",
    "criterion: 'gini', 'entropy'\n",
    "\n",
    "max_depth: 3, 5, 10, None (None means no limit)\n",
    "\n",
    "min_samples_split: 2, 5, 10\n",
    "\n",
    "min_samples_leaf: 1, 5, 10\n",
    "\n",
    "max_features: 'auto', 'sqrt', 'log2', None (None means no limit)\n",
    "\n",
    "random_state: 0, 42, 100\n",
    "\n",
    "class_weight: 'balanced', 'balanced_subsample', None (None means all classes are equal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXMALsDkvTZ2"
   },
   "source": [
    "Random Forest is a supervised learning algorithm that combines multiple decision trees to produce a more accurate and stable prediction model. It works by creating a collection of decision trees, where each tree is trained on a random subset of the training data. The final prediction is made by combining the predictions of all the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULCkDZ9bvTl0"
   },
   "source": [
    "### **The most commonly used hyperparameters for Random Forest Classifier:**\n",
    "\n",
    "n_estimators: The number of trees in the forest. Increasing this number can improve the model's performance, but also increases the computation time.\n",
    "\n",
    "criterion: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "\n",
    "max_depth: The maximum depth of each tree. Increasing this number can improve the model's performance, but also increases the risk of overfitting.\n",
    "\n",
    "min_samples_split: The minimum number of samples required to split an internal node. Decreasing this number can lead to overfitting, while increasing it can lead to underfitting.\n",
    "\n",
    "min_samples_leaf: The minimum number of samples required to be at a leaf node. Decreasing this number can lead to overfitting, while increasing it can lead to underfitting.\n",
    "\n",
    "max_features: The maximum number of features to consider at each split. Increasing this number can improve the model's performance, but also increases the computation time.\n",
    "\n",
    "max_leaf_nodes: The maximum number of leaf nodes in each tree. Increasing this number can improve the model's performance, but also increases the computation time.\n",
    "\n",
    "min_impurity_decrease: The minimum decrease in impurity required to split an internal node. Increasing this number can lead to underfitting, while decreasing it can lead to overfitting.\n",
    "\n",
    "bootstrap: Whether to use bootstrap sampling to build each tree. If True, each tree is built on a random subset of the training data.\n",
    "\n",
    "oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
    "\n",
    "random_state: The random seed used to shuffle the data before building each tree. Setting this to a fixed value ensures reproducibility of the results.\n",
    "\n",
    "class_weight: The weight assigned to each class during training. This can be useful for imbalanced datasets, where one class has a much larger number of instances than the others.\n",
    "\n",
    "### **Here are some common values for these hyperparameters:**\n",
    "\n",
    "n_estimators: 10, 50, 100, 200\n",
    "\n",
    "criterion: 'gini', 'entropy'\n",
    "\n",
    "max_depth: 3, 5, 10, None (None means no limit)\n",
    "\n",
    "min_samples_split: 2, 5, 10\n",
    "\n",
    "min_samples_leaf: 1, 5, 10\n",
    "\n",
    "max_features: 'auto', 'sqrt', 'log2', None (None means no limit)\n",
    "\n",
    "max_leaf_nodes: 10, 50, 100, None (None means no limit)\n",
    "\n",
    "min_impurity_decrease: 0.0, 0.1, 0.5\n",
    "\n",
    "bootstrap: True, False\n",
    "\n",
    "oob_score: True, False\n",
    "\n",
    "random_state: 0, 42, 100\n",
    "\n",
    "class_weight: 'balanced', 'balanced_subsample', None (None means all classes are equal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt3QA9hZxhuL"
   },
   "source": [
    "Support Vector Machine (SVM) is a supervised learning algorithm that can be used for classification and regression tasks. It works by finding the hyperplane that maximally separates the classes in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ui_ycx2xiJA"
   },
   "source": [
    "### **The most commonly used hyperparameters for Support Vector Machines (SVMs) are:**\n",
    "\n",
    "C: The regularization parameter. It controls the trade-off between the margin and the misclassification error.\n",
    "\n",
    "\n",
    "kernel: The kernel function used to transform the data into a higher dimensional space.\n",
    "\n",
    "\n",
    "gamma: The kernel coefficient. It is used to control the spread of the kernel.\n",
    "degree: The degree of the polynomial kernel.\n",
    "\n",
    "\n",
    "### **Here are some common values for these hyperparameters:**\n",
    "\n",
    "C: 1.0, 10.0, 100.0, 1000.0\n",
    "\n",
    "kernel: 'rbf', 'linear', 'poly', 'sigmoid'\n",
    "\n",
    "gamma: 'scale', 'auto', 0.1, 1.0, 10.0\n",
    "\n",
    "degree: 2, 3, 4, 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vg4EATyrOT3i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Classifier\n",
      "------------------------\n",
      "Model accuracy: 0.9070\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       999\n",
      "           1       0.92      0.89      0.91      1001\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.91      0.91      0.91      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n",
      "\n",
      "Random Forest Classifier\n",
      "------------------------\n",
      "Model accuracy: 0.9435\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       999\n",
      "           1       0.94      0.95      0.94      1001\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.94      0.94      0.94      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n",
      "\n",
      "SVM (SVC with RBF kernel)\n",
      "-------------------------\n",
      "Model accuracy: 0.9375\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94       999\n",
      "           1       0.94      0.94      0.94      1001\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.94      0.94      0.94      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # numerical computing\n",
    "from sklearn.datasets import make_classification  # synthetic dataset generator\n",
    "from sklearn.model_selection import train_test_split  # train/validation split\n",
    "from sklearn.metrics import classification_report, accuracy_score  # evaluation\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest\n",
    "from sklearn.svm import SVC  # Support Vector Classifier (SVM)\n",
    "from sklearn.pipeline import Pipeline  # to scale features before SVM\n",
    "from sklearn.preprocessing import StandardScaler  # feature scaling for SVM\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # keep notebook output clean\n",
    "\n",
    "# 2) Create a synthetic dataset: X (10,000 rows × 10 features), y (binary)\n",
    "#    - n_informative: number of truly predictive features\n",
    "#    - n_redundant: linear combos of informative features\n",
    "#    - flip_y: label noise; class_sep: separation between classes\n",
    "X, y = make_classification(\n",
    "    n_samples=10_000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=None,           # balanced classes\n",
    "    flip_y=0.01,            # 1% label noise\n",
    "    class_sep=1.0,          # default separation\n",
    "    hypercube=True,\n",
    "    shift=0.0,\n",
    "    scale=1.0,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3) Split into train/test (train_X, val_X, train_y, val_y) — 80/20\n",
    "train_X, val_X, train_y, val_y = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,          # 20% for validation\n",
    "    stratify=y,             # preserve class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Utility: uniform evaluation for each model\n",
    "def evaluate_model(model, name: str):\n",
    "    \"\"\"\n",
    "    Fits the model, predicts on validation set, computes predict_proba,\n",
    "    accuracy, and prints a classification report.\n",
    "    \"\"\"\n",
    "    # Fit\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    # Predict labels\n",
    "    val_prediction = model.predict(val_X)\n",
    "\n",
    "    # Predict probabilities (take positive class probs)\n",
    "    # For pipelines, this will call the final estimator's predict_proba\n",
    "    y_pred_proba = model.predict_proba(val_X)[:, 1]\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(val_y, val_prediction)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(f\"Model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_y, val_prediction))\n",
    "\n",
    "    # Return in case you want to reuse later\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"val_prediction\": val_prediction,\n",
    "        \"y_pred_proba\": y_pred_proba,\n",
    "        \"accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "# 4) Define three classification models with ALL hyperparameters explicitly set\n",
    "\n",
    "# 4a) Decision Tree (all key hyperparameters explicit)\n",
    "dt_clf = DecisionTreeClassifier(\n",
    "    criterion=\"gini\",          # impurity measure: \"gini\" or \"entropy\" or \"log_loss\"\n",
    "    splitter=\"best\",           # \"best\" or \"random\"\n",
    "    max_depth=10,              # limit depth to control overfitting (None = unlimited)\n",
    "    min_samples_split=2,       # min samples to split an internal node\n",
    "    min_samples_leaf=1,        # min samples required at a leaf\n",
    "    min_weight_fraction_leaf=0.0,  # min weighted fraction at a leaf\n",
    "    max_features=None,         # number of features to consider at each split (None = all)\n",
    "    random_state=42,           # reproducibility\n",
    "    max_leaf_nodes=None,       # limit number of leaves (None = unlimited)\n",
    "    min_impurity_decrease=0.0, # node split only if impurity decrease >= this\n",
    "    class_weight=None,         # e.g., \"balanced\" to handle class imbalance\n",
    "    ccp_alpha=0.0              # complexity pruning (higher -> more pruning)\n",
    ")\n",
    "\n",
    "# 4b) Random Forest (all key hyperparameters explicit)\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,          # number of trees\n",
    "    criterion=\"gini\",          # \"gini\", \"entropy\", or \"log_loss\"\n",
    "    max_depth=None,            # per-tree max depth (None = expand until pure)\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=\"sqrt\",       # features considered at each split (\"sqrt\" is common)\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,            # bootstrap samples\n",
    "    oob_score=False,           # out-of-bag scoring (requires bootstrap=True)\n",
    "    n_jobs=-1,                 # use all cores\n",
    "    random_state=42,           # reproducibility\n",
    "    verbose=0,\n",
    "    warm_start=False,          # reuse solution to add more trees\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0,             # pruning via minimal cost-complexity\n",
    "    max_samples=None           # subsample size if bootstrap=True (None = n_samples)\n",
    ")\n",
    "\n",
    "# 4c) SVM (SVC) — wrapped in a Pipeline with StandardScaler\n",
    "#     Note: probability=True enables predict_proba (via Platt scaling).\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "    (\"svc\", SVC(\n",
    "        C=1.0,                     # regularization strength\n",
    "        kernel=\"rbf\",              # \"linear\", \"poly\", \"rbf\", \"sigmoid\"\n",
    "        degree=3,                  # degree for \"poly\" kernel (ignored for \"rbf\")\n",
    "        gamma=\"scale\",             # kernel coefficient; \"scale\" or \"auto\" or float\n",
    "        coef0=0.0,                 # independent term in \"poly\"/\"sigmoid\" kernel\n",
    "        shrinking=True,            # use shrinking heuristic\n",
    "        probability=True,          # enable predict_proba\n",
    "        tol=1e-3,                  # stopping tolerance\n",
    "        cache_size=200,            # MB for kernel cache\n",
    "        class_weight=None,         # e.g., \"balanced\" for imbalance\n",
    "        verbose=False,             # debug output\n",
    "        max_iter=-1,               # -1 for no limit\n",
    "        decision_function_shape=\"ovr\",  # one-vs-rest (binary unaffected)\n",
    "        break_ties=False,          # tie-breaking in \"ovr\" (only for probability=False)\n",
    "        random_state=42            # used when probability=True for sigmoid calibration\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 5) Train & evaluate each model consistently (train_X, val_X, train_y, val_y)\n",
    "dt_results  = evaluate_model(dt_clf,    \"Decision Tree Classifier\")\n",
    "rf_results  = evaluate_model(rf_clf,    \"Random Forest Classifier\")\n",
    "svm_results = evaluate_model(svm_pipeline, \"SVM (SVC with RBF kernel)\")\n",
    "\n",
    "# 6) Access the returned dicts if you want to reuse predictions/probabilities later:\n",
    "# Example:\n",
    "# print(rf_results[\"accuracy\"])\n",
    "# print(svm_results[\"y_pred_proba\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
